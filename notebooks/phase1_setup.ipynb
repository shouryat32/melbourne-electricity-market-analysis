{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a135ebb-2008-423b-87a5-3c6af2ed6e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Notebook Parameters & Mode Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4765aa42-50b0-4b03-b65e-fe75e7624464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Running in mode: collect\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dbutils.widgets.removeAll()  \n",
    "dbutils.widgets.dropdown(\"mode\", \"collect\", [\"collect\", \"historical\", \"all\"], \"Execution Mode\")\n",
    "\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "print(f\"\uD83D\uDE80 Running in mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c76048a3-8a05-432d-9756-c308d070cf02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create database and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8fb327a-5a30-4ac9-aa56-40bff3792fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, DateType\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, \n",
    "    lit, \n",
    "    col, \n",
    "    to_date, \n",
    "    from_utc_timestamp,  \n",
    "    md5, \n",
    "    concat_ws,\n",
    "    min, max, avg, round as spark_round  \n",
    ")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS energy_analytics\")\n",
    "spark.sql(\"USE energy_analytics\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cde1f7-7ada-46fb-b880-ef0e977150fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  AEMO Live Prices - Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56abce8-bd67-4e06-99f3-68f4b6b9e688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ AEMO collection at 03:27:45\n✅ Saved 5 prices, 5 generation records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def collect_aemo_data():\n",
    "    \"\"\"Collect AEMO prices and generation every 30 minutes\"\"\"\n",
    "    import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql.functions import col, from_utc_timestamp, md5, concat_ws\n",
    "    \n",
    "    ingestion_time = datetime.now()\n",
    "    print(f\"⚡ AEMO collection at {ingestion_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    response = requests.get(\"https://visualisations.aemo.com.au/aemo/apps/api/report/ELEC_NEM_SUMMARY\", timeout=30)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Prices\n",
    "    prices = [{'region': r.get('REGIONID'), 'price': float(r.get('RRP', 0)), \n",
    "               'raise_reg': float(r.get('RAISEREGRRP', 0)), 'lower_reg': float(r.get('LOWERREGRRP', 0)),\n",
    "               'ingestion_ts': ingestion_time, 'source': 'AEMO_LIVE_API', 'settlement_date': ingestion_time.date()}\n",
    "              for r in data['ELEC_NEM_SUMMARY_PRICES']]\n",
    "    \n",
    "    df_prices = spark.createDataFrame(prices) \\\n",
    "        .withColumn(\"settlement_datetime_aest\", from_utc_timestamp(col(\"ingestion_ts\"), \"Australia/Melbourne\")) \\\n",
    "        .withColumn(\"record_hash\", md5(concat_ws(\"|\", col(\"region\"), col(\"ingestion_ts\"), col(\"price\"))))\n",
    "    \n",
    "    df_prices.write.format(\"delta\").mode(\"append\").partitionBy(\"settlement_date\", \"region\") \\\n",
    "        .saveAsTable(\"energy_analytics.bronze_aemo_live_prices\")\n",
    "    \n",
    "    # Generation\n",
    "    generation = [{'region': r.get('REGIONID'), 'total_demand': float(r.get('TOTALDEMAND', 0)),\n",
    "                   'scheduled_generation': float(r.get('SCHEDULEDGENERATION', 0)),\n",
    "                   'semischeduled_generation': float(r.get('SEMISCHEDULEDGENERATION', 0)),\n",
    "                   'net_interchange': float(r.get('NETINTERCHANGE', 0)),\n",
    "                   'ingestion_ts': ingestion_time, 'source': 'AEMO_LIVE_API', 'settlement_date': ingestion_time.date()}\n",
    "                  for r in data['ELEC_NEM_SUMMARY']]\n",
    "    \n",
    "    df_gen = spark.createDataFrame(generation) \\\n",
    "        .withColumn(\"settlement_datetime_aest\", from_utc_timestamp(col(\"ingestion_ts\"), \"Australia/Melbourne\")) \\\n",
    "        .withColumn(\"record_hash\", md5(concat_ws(\"|\", col(\"region\"), col(\"ingestion_ts\"), col(\"total_demand\"))))\n",
    "    \n",
    "    df_gen.write.format(\"delta\").mode(\"append\").partitionBy(\"settlement_date\", \"region\") \\\n",
    "        .saveAsTable(\"energy_analytics.bronze_aemo_generation\")\n",
    "    \n",
    "    print(f\"✅ Saved {len(prices)} prices, {len(generation)} generation records\")\n",
    "\n",
    "\n",
    "if mode in [\"collect\", \"all\"]:\n",
    "    collect_aemo_data()\n",
    "else:\n",
    "    print(\"⏭️  Skipping AEMO collection (mode={})\".format(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3b5a0-d6a5-434f-ae12-8a4c1d7215d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BOM Weather Data - Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14fe54e-105a-44ef-a7ee-1876c6e5ac58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDF24️  Weather collection at 03:27:55\n✅ Saved 2 weather records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_weather_data():\n",
    "    \"\"\"Collect BOM weather data every 30 minutes\"\"\"\n",
    "    import requests\n",
    "    from datetime import datetime\n",
    "    from pyspark.sql.functions import col, from_utc_timestamp, md5, concat_ws\n",
    "    \n",
    "    ingestion_time = datetime.now()\n",
    "    print(f\"\uD83C\uDF24️  Weather collection at {ingestion_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    weather_data = []\n",
    "    for station_id, product_id, station_name in [(\"95866\", \"IDV60801\", \"Melbourne Airport\"), \n",
    "                                                   (\"94866\", \"IDV60801\", \"Laverton\")]:\n",
    "        try:\n",
    "            url = f\"http://reg.bom.gov.au/fwo/{product_id}/{product_id}.{station_id}.json\"\n",
    "            response = requests.get(url, timeout=30)\n",
    "            latest = response.json()['observations']['data'][0]\n",
    "            \n",
    "            weather_data.append({\n",
    "                'station_id': station_id, 'station_name': station_name,\n",
    "                'air_temp': latest.get('air_temp'), 'apparent_temp': latest.get('apparent_t'),\n",
    "                'relative_humidity': latest.get('rel_hum'), 'wind_speed_kmh': latest.get('wind_spd_kmh'),\n",
    "                'observation_time': latest.get('aifstime_utc'),\n",
    "                'ingestion_ts': ingestion_time, 'source': 'BOM_API', 'settlement_date': ingestion_time.date()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  {station_name}: {e}\")\n",
    "    \n",
    "    if weather_data:\n",
    "        df = spark.createDataFrame(weather_data) \\\n",
    "            .withColumn(\"observation_datetime_aest\", from_utc_timestamp(col(\"ingestion_ts\"), \"Australia/Melbourne\")) \\\n",
    "            .withColumn(\"record_hash\", md5(concat_ws(\"|\", col(\"station_id\"), col(\"observation_time\"))))\n",
    "        \n",
    "        df.write.format(\"delta\").mode(\"append\").partitionBy(\"settlement_date\", \"station_id\") \\\n",
    "            .saveAsTable(\"energy_analytics.bronze_bom_weather\")\n",
    "        \n",
    "        print(f\"✅ Saved {len(weather_data)} weather records\")\n",
    "\n",
    "\n",
    "if mode in [\"collect\", \"all\"]:\n",
    "    collect_weather_data()\n",
    "else:\n",
    "    print(\"⏭️  Skipping weather collection (mode={})\".format(mode))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "phase1_setup",
   "widgets": {
    "mode": {
     "currentValue": "collect",
     "nuid": "10a61921-c572-45bd-a9d5-98f64d21cb6c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "collect",
      "label": "Execution Mode",
      "name": "mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "collect",
        "historical",
        "all"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String",
      "dynamic": false
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "collect",
      "label": "Execution Mode",
      "name": "mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "collect",
        "historical",
        "all"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}